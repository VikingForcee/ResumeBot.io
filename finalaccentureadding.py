# -*- coding: utf-8 -*-
"""FinalAccentureAdding

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-qKQWo1yPKk2VA-UZpqzxC8YV3MkJfyM
"""

pip install pdfplumber python-docx pandas

import imaplib
import email
import os

# === User Configuration ===
EMAIL = 'resumek786@gmail.com'
APP_PASSWORD = 'cmcm zjuq dxar dfbw'  # Use App Password from Google Security Settings
SEARCH_SUBJECT = 'Accenture Hiring'
DOWNLOAD_FOLDER = '/content/resumes'

# Create folder if it doesn't exist
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)

# Connect to Gmail using IMAP
mail = imaplib.IMAP4_SSL("imap.gmail.com")
mail.login(EMAIL, APP_PASSWORD)
mail.select("inbox")

# Search for emails by subject
result, data = mail.search(None, f'(SUBJECT "{SEARCH_SUBJECT}")')

mail_ids = data[0].split()

for num in mail_ids:
    result, msg_data = mail.fetch(num, "(RFC822)")
    raw_email = msg_data[0][1]
    msg = email.message_from_bytes(raw_email)

    for part in msg.walk():
        if part.get_content_maintype() == "multipart":
            continue
        if part.get("Content-Disposition") is None:
            continue

        filename = part.get_filename()
        if filename and (filename.endswith(".pdf") or filename.endswith(".docx")):
            filepath = os.path.join(DOWNLOAD_FOLDER, filename)
            with open(filepath, "wb") as f:
                f.write(part.get_payload(decode=True))
            print(f"Saved: {filename}")

mail.logout()

import os
import re
import pdfplumber
import docx
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from google.colab import files

# === Utility Functions ===

def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text

def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    return "\n".join([para.text for para in doc.paragraphs])

def extract_email(text):
    match = re.search(r'[\w\.-]+@[\w\.-]+', text)
    return match.group(0) if match else ""

def extract_phone(text):
    # Enhanced phone pattern to match multiple formats
    patterns = [
        r'(\+91[\-\s]?)?[789]\d{9}',  # Indian format
        r'(\+\d{1,3}[\-\s]?)?\(?\d{3}\)?[\-\.\s]?\d{3}[\-\.\s]?\d{4}',  # International format
        r'\d{3}[\-\.\s]?\d{3}[\-\.\s]?\d{4}'  # Simple format
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)

    return ""

def extract_name(text):
    # Get first 5 lines to look for name
    lines = text.strip().split('\n')[:5]

    # First non-empty line is likely the name
    for line in lines:
        line = line.strip()
        if line and not re.search(r'@|www|http|resume|cv', line.lower()):
            # Avoid returning full address or long descriptions
            if len(line.split()) <= 5:
                return line

    return ""

def extract_skills(text, tech_keywords=None, soft_keywords=None):
    if not tech_keywords:
        tech_keywords = [
            'python', 'java', 'javascript', 'c++', 'c#', 'ruby', 'php', 'swift', 'kotlin',
            'r', 'golang', 'scala', 'perl', 'typescript', 'html', 'css', 'react', 'angular',
            'vue', 'node.js', 'express', 'django', 'flask', 'spring', 'asp.net',
            'machine learning', 'deep learning', 'artificial intelligence', 'data science',
            'nlp', 'computer vision', 'tensorflow', 'pytorch', 'keras', 'scikit-learn',
            'pandas', 'numpy', 'scipy', 'matplotlib', 'tableau', 'power bi', 'sql', 'mysql',
            'postgresql', 'mongodb', 'oracle', 'sqlite', 'nosql', 'firebase', 'aws', 'azure',
            'gcp', 'docker', 'kubernetes', 'jenkins', 'cicd', 'git', 'github', 'linux',
            'unix', 'windows', 'macos', 'excel', 'word', 'powerpoint', 'photoshop',
            'illustrator', 'indesign', 'premiere pro', 'after effects', 'figma', 'sketch',
            'xd', 'ui/ux', 'responsive design', 'mobile development', 'web development',
            'full stack', 'frontend', 'backend', 'devops', 'cloud', 'security', 'network',
            'data analysis', 'data visualization', 'big data', 'hadoop', 'spark', 'hive',
            'blockchain', 'cryptography', 'cybersecurity', 'penetration testing',
            'system administration', 'project management', 'agile', 'scrum', 'kanban',
            'jira', 'confluence', 'trello', 'asana', 'slack'
        ]

    if not soft_keywords:
        soft_keywords = [
            'leadership', 'communication', 'teamwork', 'problem solving', 'critical thinking',
            'creativity', 'time management', 'organization', 'adaptability', 'flexibility',
            'decision making', 'conflict resolution', 'negotiation', 'presentation', 'public speaking',
            'writing', 'editing', 'research', 'analytical', 'detail oriented', 'multitasking',
            'stress management', 'work ethic', 'customer service', 'interpersonal', 'empathy',
            'emotional intelligence', 'self-motivated', 'independent', 'team player', 'collaborative',
            'mentoring', 'coaching', 'training', 'strategic thinking', 'innovation', 'entrepreneurship'
        ]

    # Function to find skill matches and handle partial word matches
    def find_skills(text, skills_list):
        found_skills = []
        text_lower = text.lower()

        for skill in skills_list:
            # Handle compound skills (contains space)
            if ' ' in skill:
                if skill.lower() in text_lower:
                    found_skills.append(skill)
            # Handle single word skills - must be whole word match
            else:
                pattern = r'\b' + re.escape(skill.lower()) + r'\b'
                if re.search(pattern, text_lower):
                    found_skills.append(skill)

        return found_skills

    tech = find_skills(text, tech_keywords)
    soft = find_skills(text, soft_keywords)

    return tech, soft

def has_linkedin(text):
    patterns = [
        r'linkedin\.com/in/[\w\-]+',
        r'linkedin\.com/[\w\-/]+',
        r'linkedin'
    ]

    for pattern in patterns:
        if re.search(pattern, text.lower()):
            return True

    return False

def has_github(text):
    patterns = [
        r'github\.com/[\w\-]+',
        r'github\.com/[\w\-/]+',
        r'github'
    ]

    for pattern in patterns:
        if re.search(pattern, text.lower()):
            return True

    return False

def extract_section(text, section_keywords, next_section_keywords=None):
    """
    Extract a section from resume text using multiple pattern matching approaches.

    Args:
        text: The resume text
        section_keywords: List of possible section headings to look for
        next_section_keywords: List of headings that might indicate the end of this section

    Returns:
        String containing the extracted section text
    """
    if next_section_keywords is None:
        next_section_keywords = [
            'EDUCATION', 'EXPERIENCE', 'WORK EXPERIENCE', 'EMPLOYMENT', 'SKILLS',
            'PROJECTS', 'ACHIEVEMENTS', 'CERTIFICATIONS', 'PUBLICATIONS', 'INTERESTS',
            'LANGUAGES', 'REFERENCES', 'SUMMARY', 'PROFILE', 'OBJECTIVE'
        ]

    # Combine all possible next section keywords
    next_sections_pattern = '|'.join([rf"\b{keyword}\b" for keyword in next_section_keywords])

    # Try different patterns to extract the section
    for keyword in section_keywords:
        # Pattern 1: Section header with colon or newline, capturing until next capital section header
        pattern1 = re.compile(
            rf"\b{keyword}\s*[:]*\s*\n(.*?)(?:\n\s*(?:{next_sections_pattern})\s*(?:\n|:)|$)",
            re.IGNORECASE | re.DOTALL
        )

        # Pattern 2: Section header with more flexible whitespace and formatting
        pattern2 = re.compile(
            rf"\b{keyword}\s*[:]*\s*(.*?)(?:\n\s*(?:{next_sections_pattern})\s*(?:\n|:)|$)",
            re.IGNORECASE | re.DOTALL
        )

        # Try both patterns
        for pattern in [pattern1, pattern2]:
            match = pattern.search(text)
            if match:
                content = match.group(1).strip()
                if content:
                    # Clean up the extracted content
                    lines = [line.strip() for line in content.split('\n') if line.strip()]
                    return ' | '.join(lines)

    # If standard patterns fail, look for sections with bullets or dashes
    for keyword in section_keywords:
        bullet_pattern = re.compile(
            rf"\b{keyword}\s*[:]*\s*\n((?:[\s\-•★·*]+[^\n]*\n)+)",
            re.IGNORECASE
        )
        match = bullet_pattern.search(text)
        if match:
            content = match.group(1).strip()
            if content:
                lines = [re.sub(r'^[\s\-•★·*]+', '', line).strip() for line in content.split('\n') if line.strip()]
                return ' | '.join(lines)

    return "NULL"

def extract_education(text):
    """
    Extract education details including degree, institution, graduation date, and GPA/percentage.
    """
    education_keywords = ["EDUCATION", "ACADEMIC BACKGROUND", "ACADEMIC QUALIFICATION", "QUALIFICATION"]

    # Get the education section
    education_section = extract_section(text, education_keywords)
    if education_section == "NULL":
        return "NULL", 0

    # Extract education score based on percentages or GPAs
    score = extract_education_score(education_section)

    return education_section, score

def extract_education_score(text):
    """
    Extract and score education based on percentages, GPAs, or degree level mentions.
    """
    # Look for percentage patterns
    percent_match = re.search(r'(\d{2,3})[\s]*%', text)
    if percent_match:
        try:
            value = float(percent_match.group(1))
            if value >= 90:
                return 5
            elif value >= 80:
                return 3
            elif value >= 70:
                return 2
            else:
                return 1
        except:
            pass

    # Look for GPA patterns
    gpa_match = re.search(r'(\d\.\d{1,2})[\s/]*(?:4|5|10)', text)
    if gpa_match:
        try:
            value = float(gpa_match.group(1))
            max_val = 4.0
            if '/10' in text or '/ 10' in text:
                max_val = 10.0
            elif '/5' in text or '/ 5' in text:
                max_val = 5.0

            normalized = (value / max_val) * 100

            if normalized >= 90:
                return 5
            elif normalized >= 80:
                return 3
            elif normalized >= 70:
                return 2
            else:
                return 1
        except:
            pass

    # Check for degree levels as a fallback
    degree_levels = {
        'phd': 5,
        'doctorate': 5,
        'master': 4,
        'mba': 4,
        'ms': 4,
        'msc': 4,
        'ma': 4,
        'bachelor': 3,
        'bs': 3,
        'ba': 3,
        'bsc': 3,
        'b.tech': 3,
        'b.e': 3,
        'associate': 2,
        'diploma': 2,
        'high school': 1,
        'secondary': 1
    }

    for degree, score in degree_levels.items():
        if re.search(rf'\b{degree}\b', text.lower()):
            return score

    # Default score if nothing is found
    return 1

def extract_experience_details(text):
    """
    Extract work experience with improved pattern matching.
    """
    experience_keywords = [
        "EXPERIENCE", "WORK EXPERIENCE", "PROFESSIONAL EXPERIENCE", "EMPLOYMENT HISTORY",
        "WORK HISTORY", "PROFESSIONAL BACKGROUND", "CAREER HISTORY"
    ]

    next_sections = [
        "EDUCATION", "SKILLS", "PROJECTS", "ACHIEVEMENTS", "CERTIFICATIONS",
        "PUBLICATIONS", "INTERESTS", "LANGUAGES", "REFERENCES"
    ]

    experience_section = extract_section(text, experience_keywords, next_sections)

    if experience_section == "NULL":
        # Try looking for company names as a fallback
        company_pattern = re.compile(r'\b(?:at|with|for)\s+([A-Z][A-Za-z\s,\.]+)(?:\s+as|\s+from|\s+\(|\s*\n)', re.MULTILINE)
        matches = company_pattern.findall(text)
        if matches:
            return ' | '.join(matches)

    # Calculate experience duration if possible
    years_of_experience = 0
    if experience_section != "NULL":
        # Look for date patterns like "2018-2020" or "Jan 2018 - Dec 2020"
        date_patterns = [
            r'(\d{4})\s*[-–—to]*\s*(\d{4}|\bpresent\b|\bcurrent\b)',
            r'([A-Za-z]+\s+\d{4})\s*[-–—to]*\s*([A-Za-z]+\s+\d{4}|\bpresent\b|\bcurrent\b)'
        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, experience_section, re.IGNORECASE)
            for match in matches:
                try:
                    start = re.search(r'\d{4}', match[0]).group(0)
                    if 'present' in match[1].lower() or 'current' in match[1].lower():
                        end = '2025'  # Assuming current year
                    else:
                        end = re.search(r'\d{4}', match[1]).group(0)

                    years_of_experience += max(0, int(end) - int(start))
                except:
                    pass

    return experience_section

def extract_projects(text):
    """
    Extract project details with improved pattern matching.
    """
    project_keywords = ["PROJECTS", "PROJECT EXPERIENCE", "ACADEMIC PROJECTS", "PERSONAL PROJECTS"]
    next_sections = [
        "EDUCATION", "EXPERIENCE", "WORK EXPERIENCE", "SKILLS", "ACHIEVEMENTS",
        "CERTIFICATIONS", "PUBLICATIONS", "INTERESTS", "LANGUAGES", "REFERENCES"
    ]

    projects_section = extract_section(text, project_keywords, next_sections)

    if projects_section == "NULL":
        # Try to find bullet points or lists that might be projects
        bullet_pattern = re.compile(r'(?:^|\n)[\s\-•★·*]+([^\n]*?(?:app|website|system|platform|database|tool|program)[^\n]*)', re.IGNORECASE)
        matches = bullet_pattern.findall(text)
        if matches:
            return ' | '.join(matches)

    return projects_section

def compute_tfidf_match(resume_text, jd_text):
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform([resume_text, jd_text])
    similarity = (tfidf_matrix[0] @ tfidf_matrix[1].T).toarray()[0][0] * 100

    if similarity >= 20:
        return similarity, 10
    elif similarity >= 15:
        return similarity, 7
    elif similarity >= 10:
        return similarity, 5
    elif similarity >= 5:
        return similarity, 3
    elif similarity >= 1:
        return similarity, 1
    return similarity, 0

# === Main Parse Function ===

def parse_resumes_with_jd(folder_path, jd_text, output_file="parsed_resumes_final.csv"):
    resume_data = []

    for filename in os.listdir(folder_path):
        if filename.lower().endswith(('.pdf', '.docx')):
            path = os.path.join(folder_path, filename)

            # Extract text
            if filename.lower().endswith('.pdf'):
                text = extract_text_from_pdf(path)
            else:
                text = extract_text_from_docx(path)

            name = extract_name(text)
            email = extract_email(text)
            phone = extract_phone(text)
            github = has_github(text)
            linkedin = has_linkedin(text)

            # Use improved extraction functions
            experience = extract_experience_details(text)
            education_section, education_score = extract_education(text)
            projects = extract_projects(text)
            achievements = extract_section(text, ["ACHIEVEMENTS", "ACCOMPLISHMENTS", "AWARDS"])

            tech_skills, soft_skills = extract_skills(text)
            tfidf_score, jd_points = compute_tfidf_match(text, jd_text)

            # Calculate score
            score = 0
            score += 10 if name else 0
            score += 10 if email else 0
            score += 10 if phone else 0
            score += 3 if github else 0
            score += 3 if linkedin else 0
            score += education_score
            score += 5 if experience != "NULL" else 0
            score += 5 if projects != "NULL" else 0
            score += len(tech_skills) * 2.5
            score += len(soft_skills) * 0.5
            score += jd_points

            if(score > 65):
                status = "Shortlisted"
            elif (score > 40):
                status = "To be Reviewed"
            else:
                status = "Rejected"

            resume_data.append({
                'Filename': filename,
                'Name': name,
                'Email': email,
                'Phone': phone,
                'LinkedIn': linkedin,
                'GitHub': github,
                'EducationScore': education_score,
                'Education': education_section,
                'TechSkills': ', '.join(tech_skills),
                'SoftSkills': ', '.join(soft_skills),
                'Experience': experience,
                'Achievements': achievements,
                'Projects': projects,
                'JD_Match_Score': round(tfidf_score, 2),
                'Status': status,
                'Final_Score': round(score, 2)
            })

    df = pd.DataFrame(resume_data)
    df.to_csv(output_file, index=False)
    print(f"✅ Parsing complete. Saved as '{output_file}'.")
    return df

# === Colab Specific Functions ===

def upload_jd_file():
    print("Please upload your JD file (PDF, DOCX, or TXT):")
    uploaded = files.upload()

    if not uploaded:
        raise ValueError("No file was uploaded.")

    file_path = list(uploaded.keys())[0]

    # Extract text from JD file based on file extension
    jd_text = ""
    if file_path.lower().endswith('.pdf'):
        print(f"Extracting text from JD PDF: {file_path}")
        jd_text = extract_text_from_pdf(file_path)
    elif file_path.lower().endswith('.docx'):
        print(f"Extracting text from JD DOCX: {file_path}")
        jd_text = extract_text_from_docx(file_path)
    else:
        print(f"Reading JD from text file: {file_path}")
        with open(file_path, "r", encoding="utf-8") as f:
            jd_text = f.read()

    # Check if JD was successfully extracted
    if not jd_text:
        raise ValueError(f"Could not extract text from JD file: {file_path}")

    print(f"Successfully extracted JD text ({len(jd_text)} characters)")
    return jd_text

# Use this function to run the resume parser in Colab
def run_resume_parser(resume_folder="resumes"):
    # First, upload the JD file
    jd_text = upload_jd_file()

    # Check if resume folder exists
    if not os.path.exists(resume_folder):
        print(f"⚠️ Error: Resume folder '{resume_folder}' does not exist.")
        return

    # Parse resumes with the extracted JD
    results_df = parse_resumes_with_jd(resume_folder, jd_text)

    # Download the results
    files.download("parsed_resumes_final.csv")

    return results_df

# Call this function to start the process
# run_resume_parser("resumes")

run_resume_parser("resumes")  # Replace "resumes" with your actual folder path

# Install required libraries (pandas only needed for CSV)
!pip install pandas

# Import necessary libraries
import pandas as pd
import smtplib
from email.mime.text import MIMEText
from google.colab import files
import time

# === Email Configuration ===
SENDER_EMAIL = "resumek786@gmail.com"
SENDER_PASSWORD = "cmcm zjuq dxar dfbw"  # Your App Password

def send_email(to_email, candidate_name, status):
    # [Same send_email function as above, unchanged]
    if status == "Shortlisted":
        subject = "Congratulations - You've Been Shortlisted!"
        body = f"""Dear {candidate_name},

We are pleased to inform you that you have been shortlisted for the position at Accenture.
Your skills and experience align well with our requirements.

Next Steps:
- We will contact you soon regarding the interview process
- Please keep your schedule flexible for the coming week

Best regards,
Hiring Team
Accenture
"""
    elif status == "Rejected":
        subject = "Application Update from Accenture"
        body = f"""Dear {candidate_name},

Thank you for applying to Accenture. After careful consideration, we regret to inform you
that we will not be moving forward with your application at this time.

We appreciate your interest in our company and the time you invested in applying.
We encourage you to apply for future positions that match your skills and experience.

Best wishes,
Hiring Team
Accenture
"""
    else:
        return False

    msg = MIMEText(body)
    msg['Subject'] = subject
    msg['From'] = SENDER_EMAIL
    msg['To'] = to_email

    try:
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(SENDER_EMAIL, SENDER_PASSWORD)
        server.send_message(msg)
        server.quit()
        print(f"Email sent successfully to {to_email} - Status: {status}")
        return True
    except Exception as e:
        print(f"Failed to send email to {to_email}: {str(e)}")
        return False

def process_csv_and_send_emails():
    # Step 1: Upload the CSV file
    print("Please upload your CSV file containing candidate data")
    uploaded = files.upload()

    if not uploaded:
        print("No file uploaded. Process aborted.")
        return

    # Step 2: Read the CSV file
    filename = list(uploaded.keys())[0]
    try:
        df = pd.read_csv(filename)
    except Exception as e:
        print(f"Error reading CSV file: {str(e)}")
        return

    # Step 3: Verify required columns
    required_columns = ['Name', 'Email', 'Status']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Error: Missing required columns: {missing_columns}")
        return

    # Step 4: Process each row and send emails
    email_count = 0
    for index, row in df.iterrows():
        name = row['Name'] if pd.notna(row['Name']) else "Candidate"
        email = row['Email']
        status = str(row['Status']).strip() if pd.notna(row['Status']) else ""

        if pd.isna(email) or not email:
            print(f"Skipping row {index + 2}: No valid email address")
            continue

        if status in ["Shortlisted", "Rejected"]:
            success = send_email(email, name, status)
            if success:
                email_count += 1
            time.sleep(2)  # 2-second delay between emails
        else:
            print(f"Skipping {email}: Status '{status}' is not 'Shortlisted' or 'Rejected'")

    print(f"\nProcess completed! Sent {email_count} emails successfully.")

# Run the script
if __name__ == "__main__":
    process_csv_and_send_emails()